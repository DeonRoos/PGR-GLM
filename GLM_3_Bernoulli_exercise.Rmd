---
title: 'Day 3: Bernoulli GLMs'
# author: Deon Roos
output: 
  html_document:
    toc: false
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r setup, echo=FALSE, purl = FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE, eval = FALSE, cache = FALSE)

SOLUTIONS <- FALSE
```

\  

# Bernoulli GLM - Predicting if Netflix users will watch Lord of the Rings: The Fellowship of the Ring

\  

For the GLM exercises, we'll use the workflow we suggested in the first GLM overview lecture as a template, specifically:

1. Know your research question!

2. Think about your response variable (stochastic).

3. Think about the process behind the data (deterministic).

4. Understand the data that you’ve collected (plot it!)

5. Combine into a model that can answer your question.

6. Fit the model.

7. Check your assumption(s).

8. Answer your question.

\

### 1. Know your research question!

\

For this final exercise, we'll do something that's, hopefully, a bit fun and silly. We're going to work with a relatively small Netflix dataset, where we're going to try and *predict* if a user will will watch Lord of the Rings: The Fellowship of the Ring. I will admit to a number of ulterior motivations in ending the course using this dataset.

1. There is a common misconception that Machine Learning is for predicting and Statistics is for understanding. I want to show that this distinction is meaningless. In truth, machine learning is statistics; just performed (or in some cases developed) by computer scientists. For example, the GLMs you've learned about on this course are also called Supervised Machine Learning Models. The distinction between the two methods is meaningless, so I wanted to use an example where we do prediction to show just that.

2. Because people tend to think of there being some sort of distinction between Machine Learning and Statistics, it can lead to people with highly desirable skills, not even realising they have the exact skills that lots of employers are looking for (including non-academic). By using this Netflix example I want to show that, having taken this course, your employment options have broadened.

3. In the final lecture, I show an example where AIC fails miserably. In doing so, I'm at risk of leaving you with the impression that AIC should never be used. Here, we use an example where AIC is wonderfully well suited for helping us in our objective.

4. I think this helps understand how information that corporations collect about you can be used and also how the methods you use are part of the Artificial Intelligence hype train that we're currently on. However, to be clear and to avoid underselling some of the more advanced Machine Learning models, some Machine Learning models are far more sophisticated than the GLMs we've been running on this course.

5. I really like Lord of the Rings.

6. Like, a lot.

> While this exercise takes on an industry focus, this is not specific to $Bernoulli$ GLMs. $Bernoulli$ GLMs can be used to answer a variety of research questions in the same way as GLMs using $Poisson$, $Binomial$, and any other distribution.

The data you've been provided includes various information that Netflix has on hand about their users, for the purposes of predicting which films, shows or media users will engage with. Netflix are not interested in understanding causation or writing a scientific paper, they want predictive power.

For this dataset, the objective is predicting which type of user, who has never seen Lord of the Rings (LoTR), are more or less likely to watch LoTR. To do so, our response variable if whether not a given viewer watched LoTR (1) or not (0).

The covariates we have are:
* `user` - a unique user ID
* `lotr` - if that user watched LoTR on Netflix
* `premium` - if the user has Netflix Premium or Standard
* `age` - the age of the user at the time of data collection
* `genre_likes` - how many fantasy films/shows the user had previously liked
* `actor_likes` - how many films/shows the user had previously liked that included cast members of LoTR
* `similar_user_ratins` - what lotr of "similar" users had liked LoTR ("similar" is undefined)
* `mean_time_genre` - what is the average length of time users watch fantasy media
* `user_time` - Time of day for user at time of data collection (`Morning`, `Afternoon` and `Evening`)
* `user_day` - Day of week for user at time of data collection (`Monday`, `Tuesday`, etc.)
* `user_device` - Type of device user was logged in from (`TV`, `PC`, `Mobile Device` and `Gaming Device`)
* `fam_members` - Number of family member accounts tied to subscription
* `country` - Country of subscription

As concisely as possible, write down the research question that can be answered with our analysis.

```{r Q1, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
# Using the available covariates, can Lord of the Ring engagement be predicted?
```

\

### 2. Think about your response variable (the stochastic element).

\

From the information provided in the description above, we can determine that a $Bernoulli$ is a sensible distribution to use here; our data is 0 (user did not watch LoTR) or 1 (user did watch LoTR). Our stochastic element of the model would therefore be:

$y_i \sim Bernoulli(p_i)$

Where $y$ is if user $i$ watched LoTR or not, generated according to a $Bernoulli$ distribution, with probability $p$.

\

### 3. Think about the process behind the data (the deterministic element).

\

Just like the drinking coffee example from the first lecture, the *Data Generating Process* (DGP) for whether or not and why someone watches a partnetflixlar film on Netflix or not is going to be incredibly complex. A nuance here is that objective and intended use for this analysis is prediction. When that's the case, we can be a bit more relaxed about the DGP and add any and all covariates for which we have data, with an important caveat. We must be relatively comfortable that we have not included any covariates which have no conceivable connection to our response. For example, we wouldn't want to include average sea surface temperature because, hopefully obviously, it won't give us any true predictive power. But if we're unlucky, it may be correlated (by pure chance) with LoTR viewership. In this data, Netflix have collected the data and covariates for the explicit purpose of trying to predict, so we can be confident that at least *someone* thinks these variables might give us some predictive power.

With that said, our model will still have a deterministic equation that describes the associations. Given how many variables we have it would be cruel to task you with writing the equation out in full. Instead, assume our model was `lotr ~ age + user_time`. Use this to write out the equation that would underpin this model. As a reminder, `age` is a continuous variable, and `user_time` is a categorical variable with the levels `Morning`, `Afternoon` and `Evening`.

For this equation, I'm not giving any hints so the diffnetflixlt does spike here. If you're struggling, ask for help. It really is worthwhile learning how the syntax `lotr ~ age + user_time` is translated into an equation.

$logit(p_i) = \text{___}$

```{r Q3, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
# With the blanks filled in, the equation should be:
# logit(p_i) = beta_0 + beta_1 * Age_i + beta_2 * Evening_i + beta_3 * Morning_i

# Why have I included Evening and then Morning, and not Afternoon?
# Because R will take the level that appears first alphanumerically, and use 
# that as the intercept (here *A*fternoon)
# If we wanted Morning to be the reference level, what R code would we use?
```

\  

### 4. Understand the data that you’ve collected (plot it!)

\

Import the data file 'netflix.txt' into R and take a look at the structure of this dataframe and create plots you feel will help you (and others) understand the data.

Keep the tips and tricks you used in yesterday's practical and feel free to use them again here; load in the data, check for any covariates you need to adjust, plot out the data, etc.

A bit of advice for visualising $Bernoulli$ data; adding a little noise to the x and y-axis values for each point (i.e. jittering) helps immensely for visualising. To do so, the code would be:

```
plot(jitter(my_data$y) ~ my_data$x)
```

As in yesterday's lectures, the figures below are produced using `ggplot2` and are included purely for inspiration. I am also using an additional package, `patchwork`, to stitch together multiple figures into a single panel.

```{r Q4, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
netflix <- read.table(file= "./data/netflix.txt", header = TRUE)

# Change variables from characters to factors
netflix$user <- factor(netflix$user)
netflix$premium <- factor(netflix$premium)
netflix$user_time <- factor(netflix$user_time, levels = c("Morning", "Afternoon", "Evening"))
netflix$user_day <- factor(netflix$user_day, 
                           levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
netflix$user_device <- factor(netflix$user_device)
netflix$country <- factor(netflix$country)

str(netflix)
# 'data.frame':	15000 obs. of  13 variables:
#  $ user                : Factor w/ 15000 levels "0014074f-9304-4349-b1df-565122630bfa",..: 13625 2447 4081 ...
#  $ lotr                : int  0 1 1 1 0 0 0 0 0 1 ...
#  $ premium             : Factor w/ 2 levels "Premium","Standard": 2 2 2 2 2 2 2 2 2 1 ...
#  $ age                 : int  30 26 27 39 37 37 43 40 36 38 ...
#  $ genre_likes         : int  2 2 2 2 0 2 1 5 2 5 ...
#  $ actor_likes         : int  3 2 4 0 4 3 2 0 4 6 ...
#  $ similar_user_ratings: num  0.62 0.67 0.92 0.75 0.46 0.59 0.77 0.62 0.67 0.54 ...
#  $ mean_time_genre     : num  38.3 66.9 118.4 217.4 127.8 ...
#  $ user_time           : Factor w/ 3 levels "Afternoon","Evening",..: 1 2 1 1 2 1 2 2 2 3 ...
#  $ user_day            : Factor w/ 7 levels "Friday","Monday",..: 5 2 3 4 1 3 1 4 3 3 ...
#  $ user_device         : Factor w/ 4 levels "Gaming Device",..: 4 4 4 4 4 4 4 4 1 2 ...
#  $ fam_members         : int  3 3 2 0 1 2 1 2 2 1 ...
#  $ country             : Factor w/ 7 levels "DE","FR","JP",..: 2 6 7 5 7 6 2 6 6 4 ...

summary(netflix)
 #                                   user            lotr            premium           age         genre_likes   
 # 0014074f-9304-4349-b1df-565122630bfa:    1   Min.   :0.0000   Premium : 3001   Min.   : 7.00   Min.   :0.000  
 # 0017781d-22a6-41d1-9c61-10427b20f846:    1   1st Qu.:0.0000   Standard:11999   1st Qu.:24.00   1st Qu.:1.000  
 # 001f0d02-282e-4812-87b3-723e978da027:    1   Median :1.0000                    Median :29.00   Median :2.000  
 # 0021ddf2-ef09-4abe-b27d-d3fa74fd92b7:    1   Mean   :0.5278                    Mean   :29.98   Mean   :1.998  
 # 0023c3d4-ba8f-4716-8c30-6d6859c77b85:    1   3rd Qu.:1.0000                    3rd Qu.:35.00   3rd Qu.:3.000  
 # 00295ab4-6930-4d7e-b678-b10a65c0f234:    1   Max.   :1.0000                    Max.   :79.00   Max.   :9.000  
 # (Other)                             :14994                                                                    
 #  actor_likes     similar_user_ratings mean_time_genre     user_time         user_day           user_device   
 # Min.   : 0.000   Min.   :0.0300       Min.   :  0.0   Afternoon:4419   Friday   :2724   Gaming Device: 1535  
 # 1st Qu.: 2.000   1st Qu.:0.5300       1st Qu.: 79.9   Evening  :9104   Monday   :1698   Mobile Device: 2066  
 # Median : 3.000   Median :0.6900       Median :120.1   Morning  :1477   Saturday :2792   PC           : 1223  
 # Mean   : 2.973   Mean   :0.6661       Mean   :120.5                    Sunday   :3150   TV           :10176  
 # 3rd Qu.: 4.000   3rd Qu.:0.8300       3rd Qu.:160.3                    Thursday :1805                        
 # Max.   :12.000   Max.   :1.0000       Max.   :344.2                    Tuesday  :1352                        
 #                                                                        Wednesday:1479                        
 #  fam_members    country  
 # Min.   :0.000   DE:2476  
 # 1st Qu.:1.000   FR:2119  
 # Median :2.000   JP: 302  
 # Mean   :1.992   MX:1753  
 # 3rd Qu.:3.000   NZ: 728  
 # Max.   :9.000   UK:2825  
 #                 US:4797  

# install.packages("ggplot2") # Run this line of code if you do not have ggplot installed
# Once installed, load the package
library(ggplot2)

# I'll use another package called `patchwork` that helps combine ggplot figure together
# install.packages("patchwork") # Run this line of code if you do not have patchwork installed
# Once installed, load the package
library(patchwork)

# Not needed but downloading netflix icon to add to figure to give that corporate feel
# If you want to do this, you'll need to install the `png` package
logo <- png::readPNG("./images/netflix.png")

# Bar chart showing total number of views according to account type
p1 <- ggplot(netflix) +
  geom_col(aes(x = premium, y = lotr), linetype = 1,
           alpha = 0.8, colour = "#E50914") +
  labs(x = "Subscription type",
       y = "Total number of\nLord of The Ring views") +  
  theme_classic() +
  annotation_raster(logo, 
                    xmin = 0.5, xmax = 1,
                    ymin = 4000, ymax = 6000)

# A scatterplot for lotr ~ age, jittered
p2 <- ggplot(netflix) +
  geom_jitter(aes(x = age, y = lotr), 
              width = 0, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "User age",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# A scatterplot for lotr ~ genre_likes, jittered
p3 <- ggplot(netflix) +
  geom_jitter(aes(x = genre_likes, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Number of fantasy likes",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# A scatterplot for lotr ~ actor_likes, jittered
p4 <- ggplot(netflix) +
  geom_jitter(aes(x = actor_likes, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Number of films with same actor liked",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# Using the patchwork package I stich p1 to p4 together into a single figure
(p1 + p2) / (p3 + p4)

# A scatterplot for lotr ~ similar_user_ratings, jittered
p5 <- ggplot(netflix) +
  annotation_raster(logo, 
                    xmin = -0.05, xmax = 0.25,
                    ymin = 0.7, ymax = 1.4) +
  geom_jitter(aes(x = similar_user_ratings, y = lotr), 
              width = 0, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Proportion of similar users who liked LoTR",
       y = "Viewed\nLord of The Ring") +
  theme_classic()


# A scatterplot for lotr ~ mean_time_genre, jittered
p6 <- ggplot(netflix) +
  geom_jitter(aes(x = mean_time_genre, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Mean time spent watching fantasy films",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# A scatterplot for lotr ~ user_time, jittered
p7 <- ggplot(netflix) +
  geom_jitter(aes(x = user_time, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Time of day",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# A scatterplot for lotr ~ user_day, jittered
p8 <- ggplot(netflix) +
  geom_jitter(aes(x = user_day, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Day of week",
       y = "Viewed\nLord of The Ring") +
  theme_classic()


# Using the patchwork package I stich p1 to p4 together into a single figure
(p5 + p6) / (p7 + p8)

# A scatterplot for lotr ~ fam_members, jittered
p9 <- ggplot(netflix) +
    annotation_raster(logo,
                      xmin = 8.3, xmax = 9.6,
                      ymin = -0.3, ymax = 0.5) +
  geom_jitter(aes(x = fam_members, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Number of accounts",
       y = "Viewed\nLord of The Ring") +
  theme_classic()
  
# A scatterplot for lotr ~ user_device, jittered
p10 <- ggplot(netflix) +
  geom_jitter(aes(x = user_device, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "User device",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# A scatterplot for lotr ~ country, jittered
p11 <- ggplot(netflix) +
  geom_jitter(aes(x = country, y = lotr), 
              width = 0.3, height = 0.3,
              colour = "#E50914", alpha = 0.05) +
  scale_y_continuous(labels = function(y) ifelse(y == 0, "No", ifelse(y == 1, "Yes", y)),
                     breaks = c(0, 1)) +
  labs(x = "Country of registration",
       y = "Viewed\nLord of The Ring") +
  theme_classic()

# Using the patchwork package I stich p1 to p4 together into a single figure
p9 / (p10 + p11)
```

\

### 5. Combine into a model that can answer your question.

\

Having gone through the previous steps, it's now time to run our first model. We'll now run a model using all covariates that we've been provided.

Run the model now, using `glm()`.

* Hints:
  + Because $Bernoulli$ is really just a special case of the $Binomial$ distribution, we specify `family = binomial`
  + Do we need to specify number of success and number of failures like we did for $Binomial$ GLMs?
  + What is the default link function used by $Bernoulli$ GLMs?

```{r Q5, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
mod1 <- glm(lotr ~ premium + age + genre_likes + actor_likes + similar_user_ratings +
              mean_time_genre + user_time + user_day + user_device + fam_members + country, 
            family = binomial(link = "logit"), 
            data = netflix)
```

\

### 6. Check your assumption(s).

\

Having run the model, we want to check how well we're meeting the assumptions.

To do so, we can check the model diagnostic plots, as well as check for dispersion. Do so now.

For the diagnostic plots:

+ Residuals vs Fitted
  * What kind of pattern would we expect?
+ Q-Q Residuals
  * Are we expecting Normally distributed error with a Poisson distribution?
+ Scale-Location
  * What kind of pattern would we expect?
+ Residuals vs Leverage
  * Are any observations having a strong influence on the model fit?

```{r Q6, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
# Model diagnostic plots:
# Residuals vs Fitted
  # We see some possible patterns going on in the data but for the most part
  # we see relatively constant variation. I am slightly concerned about the "pattern", but 
  # I'll use the Scale-Location plot to help me gauge how if I should be concerned.
  # Specifically, the apparent "pinching" that occurs at a predicted value of ca.
  # -1 (i.e. -1 on the logit link scale, or ca. 25% probability). This may indicate
  # a problem but it may just be because we don't have many observations with that
  # predicted value.
  # Note that the "streak" of residuals at the bottom left of the figure, going 
  # from ca. -2 to -5 (below the dashed line) are instances where we have low
  # predicted logit values, hence low probabilities, so we are approaching the lower
  # bound of the data (i.e. 0%). In these scenarios, we're always going to see a "streak"
  # of residual values that tend towards 0 residual error.
# Q-Q Residuals
  # We completely ignore this figure for GLMs.
# Scale-Location
  # There seems to be less obvious patterns when we view the data here, compared
  # to when we look at the Residuals vs Fitted figure. I am now more comfortable
  # that there's not much going on in the residuals, in terms of patterns.
# Residuals vs Leverage
  # We're really only using this to check for values close to a Cook's distance of 1
  # For this plot, we can just make out the 0.5 dashed line in the top right, so 
  # we have no highly influential points in the data.
# Overall, the diagnostic plots look ok. Not perfect, but good enough that I'd be 
  # comfortable interpreting the results.

# To do a quick and crude check for dispersion, we can use the information from summary()
  # We take residual deviance and divide by the degrees of freedom, so for this model:
  # 229.78/205 = 1.12
  # We have very, very slight overdispersion but nothing that causes me concern!

# From these diagnostic checks, I see no issue for why we can't proceed with interpretation.

# While you might be suspicious that the first model we ran apparently has no issues. 
  # This does happen with real analysis. The risk when this happens, though, is that 
  # sometimes researchers feel compelled to run alternative models to try and "break" 
  # things, or think "maybe we can get away with adding a bit more complexity". I'd 
  # argue neither are appropriate. We set out to answer a question and we've now confirmed,
  # to the best of our ability, that our model is sound. There might be other issues
  # (e.g. the assumption of validity) but this model is perfectly adequate for 
  # doing what we wanted it to do. We're more than justified in running this one
  # model without torturing ourselves further.

par(mfrow = c(2,2)) # Show figures in 2 rows and 2 columns
plot(mod1)          # Plot the model diagnostics
par(mfrow = c(1,1)) # Reset so only 1 figure is shown
summary(mod1)       # Get the summary of the model (to check dispersion)
```

\

### 7. Interpret your model.

\

Using `summary()`, go through each `Estimate` and provide an interpretation of what it means scientifically. E.g. what does the `n_staff` `Estimate` mean?

```{r Q7, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE}
summary(mod1)
#                               Estimate Std. Error z value Pr(>|z|)    
# (Intercept)                   -1.90007    0.60588  -3.136  0.00171 ** 
# n_staff                       -0.12192    0.02744  -4.442 8.90e-06 ***
# policyNot implemented          1.04076    0.58723   1.772  0.07634 .  
# capacity                       2.37859    0.34223   6.950 3.65e-12 ***
# n_staff:policyNot implemented  0.07109    0.02930   2.426  0.01525 *  

# (Intercept)
  # This is the predicted value (on the link scale) when all other covariates
  # are set to zero. For "policy", this is when the policy *was* implemented. We know
  # that because the "policy" Estimate (further down) is for "Not implemented", 
  # meaning R has "created" a column of data called "Not implemented" and used 1 to 
  # mean "Yes" and 0 to mean "No". Why did R choose "Implemented" to be our reference? 
  # Because "I" is before "N" in the alphabet.
  # The Estimate value of -1.9 means that when capacity is 0, n_staff
  # is 0, and the policy was not implemented, we estimate -1.9 patients would be
  # MRSE positive (on the link scale).
# n_staff
  # This is the predicted slope for n_staff on the link scale *when the policy was
  # implemented. This caveat comes about because we have an interaction between 
  # policy and n_staff. This means we interpret this Estimate as: for every additional
  # member of staff in a ward *when the policy was implemented*, the lotr 
  # of patients with MRSE *decreases* by -0.12 (we know it decreases because the
  # estimate is negative).
# policyNot implemented
  # This is the difference between when the policy is *not implemented*, compared
  # to when it is implemented. Here, the value of 1.04 means that when the policy
  # is not implemented, we would expect *more* patients to be MRSE positive.
  # Specifically, -1.90007 + 1.04076 (Intercept + policyNot implemented).
  # This would imply the policy is effective.
# capacity
  # This is the slope for ward capacity. For every additional unit increase in capacity
  # the number of MRSE patients *increases* by 2.37. We need to be a little careful
  # here and remind ourselves what a unit increase in capacity represents. Capacity
  # is the lotr of beds occupied in a ward, meaning it can vary between 0
  # and 1. So a unit increase is going from 0% to 100% - from nothing to everything.
  # If we wanted the capacity Estimate to be more intuitive, we could multiple the
  # explanatory variable (in our mrse dataframe) by 100 and rerun our analysis. 
  # If we did so, a unit increase would become an increase in 1%, rather than 100%.
# n_staff:policyNot implemented
  # This is our interaction, which in this case, is the difference in slope 
  # between the slope for n_staff when the policy was implemented (n_staff Estimate) 
  # with the compared to the slope for n_staff when we do not implement the policy.
  # *Importantly*, the slope is not 0.07109. Remember, it's the difference in slope
  # compared to the slope for n_staff when the policy was implemented. Here, the
  # slope is: -0.12192 + 0.07109 = -0.05083
```

\

### 8. Create figures to show predicted relationships.

\

In the $Poisson$ exercise, we relied on `packages` to make our figures for us. In today's exercise, we'll do these ourselves so that we get the freedom to make any aesthetic changes we might want, or to make show predictions according to specific circumstances. Doing so, inevitably, will require more work on our side though.

For started, we need a "new" dataset that contains an even spread of covariate values that we want to use to make predictions. In the lecture, I showed a trick we can use to do so; by using the `expand.grid()` function. Using this we'll start off by making a dataset to show the relationship for `capacity`, and then afterwards, do this again to show the relationship for the interaction between `policy` and `n_staff`. Regardless of which prediction we are doing, we must provide values for all covariates in the model, even those we don't necessarily want to show. The choice for what values to set covariates to is not necessarily trivial, but the convention is to set them to the median. For example, for `n_staff`, we might set all values of this covariate to `median(netflix$n_staff)`, but there's nothing to stop us from setting these to values we are partnetflixlarly interested in. For instance, maybe we want to show the predicted relationship of capacity with the maximum number of staff if we wanted to show the best case scenario for the capacity relationship. Now that we're creating these predictions by hand, rather than relying on `R` packages, we have the freedom to do so.

For `capacity`, we can create our fake dataset using the following code:

```
# Create a fake dataset to feed into our model equation
synth_data <- expand.grid(
  # Set n_staff to median (22 staff) or any value you think is interesting
  n_staff = median(netflix$n_staff), 
  # We have to set policy to either Not implemented or Implemented
  policy = "Implemented",        
  capacity = seq(
    # We create a sequence from the minimum capacity observered
    from = min(netflix$capacity), 
    # To the maximum capacity observed
    to = max(netflix$capacity),   
    # And request 20 values across that range
    length.out = 20))         
```

We can then use this new *synthetic* dataset to create predictions of lotr of MRSE positive patients using the `predict()` function. The `predict()` function takes our model equation (with the estimated parameter values) and combines this with either the original data (if we did not supply a synthetic dataset) or with a synthetic data (if you do supply a synthetic dataset) to calculate the expected MRSE lotrs, given the respective covariate values.

```
synth_data$pred <- predict(mod1, newdata = synth_data)
```

A hint: we can tell the `predict()` function to use the inverse link function automatically when making the predictions, so we don't need to by setting `type = "response"` or else we could always do this manually by using the inverse link function (here $\frac{e^x}{(1+e^x)}$, or `plogis()` in `R`) such that our predicted values are on the response scale. 

```
synth_data$pred <- predict(mod1, newdata = synth_data, type = "response")
# Or
synth_data$pred <- plogis(predict(mod1, newdata = synth_data))
```

Once done, we can use our new synthetic data and predicted MRSE prevalence to create a figure that shows the relationships for capacity.

```
# Plot the raw data
plot(netflix$capacity, netflix$lotr, xlab= "Ward capacity", ylab= "lotr of patients with MRSE")
# And add a line to show our predicted model fit
lines(synth_data$pred ~ synth_data$capacity, lty = 1, col = 1)
```

Your figure should look like:

```{r Q8.1, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE, fig.height=8, fig.show= ifelse(TRUE, "asis", "hide")}
# Create a fake dataset to feed into our model equation
synth_data <- expand.grid(
  # Set n_staff to median (22 staff)
  n_staff = median(netflix$n_staff), 
  # We have to set policy to either Not implemented or Implemented
  policy = "Implemented",        
  capacity = seq(
    # We create a sequence from the minimum capacity observered
    from = min(netflix$capacity), 
    # To the maximum capacity observed
    to = max(netflix$capacity),   
    # And request 20 values across that range
    length.out = 20))         

# Get predictions on the response scale
synth_data$pred <- predict(mod1, newdata = synth_data, type = "response")
# Or
# synth_data$pred <- plogis(predict(mod1, newdata = synth_data))

# Plot the raw data
plot(netflix$capacity, netflix$lotr, xlab= "Ward capacity", ylab= "lotr of patients with MRSE")
# And add a line to show our predicted model fit
lines(synth_data$pred ~ synth_data$capacity, lty = 1, col = 1)
```

Now for the harder part of the model. We'll need to go through the same steps as above, but this time making tweaks to show the interaction between `policy` and `n_staff`.

Hint: the biggest "part" here is creating the synthetic data such that we can show the `n_staff` relationship for *both* levels of `policy`, but doing so, requires only two small changes to the `expand.grid()` code.

Give it a go, but if you're struggling, ask for help. This is a tough one.

```{r Q8.2, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE, fig.height=8, fig.show= ifelse(SOLUTIONS, "asis", "hide")}
# Create a fake dataset to feed into our model equation
synth_data <- expand.grid(n_staff = seq(from = min(netflix$n_staff), to = max(netflix$n_staff), length.out = 20), 
                          policy = c("Implemented", "Not implemented"),
                          capacity = 0.5)

synth_data$pred <- predict(mod1, newdata = synth_data, type = "response")

plot(netflix$n_staff, netflix$lotr, col= netflix$policy, xlab= "Number of staff", ylab= "lotr of patients with MRSE")

lines(synth_data$pred[synth_data$policy == "Implemented"] ~ 
      synth_data$n_staff[synth_data$policy == "Implemented"], lty= 1, col= 1)
lines(synth_data$pred[synth_data$policy == "Not implemented"] ~ 
      synth_data$n_staff[synth_data$policy == "Not implemented"], lty= 1, col= 2)

legend("topright", 
 legend= c("Implemented", "Not implemented"), 
 col= c(2:1), 
 lty= c(1, 1, 1), 
 lwd= c(1, 1, 1))
```

\

### 9. Uncertainty

\

It's always important to show uncertainty when visualising (or reporting) what our model has estimated. Without uncertainty, we essentially tell our reader that we know *exactly* how the world works. Clearly we don't, we have a crude understanding build upon many assumptions. Showing uncertainty, often through confidence intervals, is a way to, at the very least, acknowledge that we *don't* know the exact Truth.

Including confidence intervals requires that we calculate them. Doing so by hand is an absolute pain, but thankfully, `R` makes this relatively easier. All we need to do is tell the `predict()` function to also report standard error for each predicted observation, and then convert these to upper and lower 95% CI.

Here's how we do that for capacity:

```
# We remake our synthetic data for visualising *capacity*
synth_data <- expand.grid(
  n_staff = median(netflix$n_staff), 
  policy = "Implemented",        
  capacity = seq(
    from = min(netflix$capacity), 
    to = max(netflix$capacity),   
    length.out = 20))         

# Now instead of just getting a single column of mean prediction, we ask predict()
  # for two columns - one for mean prediction and another for standard error.
  # This means we cannot simply add the predictions, as is, to our synth_data
  # Instead we store the output to then calculate 95% CI
# Note that we now include the argument se.fit = TRUE, to get our standard error
  # We also cannot use type = response anymore, instead we use plogis but later on
pred <- predict(mod1, newdata = synth_data, se.fit = TRUE)

# Extract the mean Estimate on response scale
synth_data$pred <- plogis(pred$fit)
# Subtract SE * 1.96 to get 95% and backtransform for lower 95% CI
synth_data$low <- plogis(pred$fit - pred$se.fit * 1.96)
# Add SE * 1.96 to get 95% and backtransform for upper 95% CI
synth_data$upp <- plogis(pred$fit + pred$se.fit * 1.96)

# Plot the raw data
plot(netflix$capacity, netflix$lotr, 
     xlab = "Ward capacity", 
     ylab = "lotr of patients with MRSE")
# As before, add a line to show our predicted model fit
lines(synth_data$pred ~ synth_data$capacity, lty = 1, col = "black")
# Now we add two new dashed lines (lty = 2 for dashed) to show 95% CI
lines(synth_data$low ~ synth_data$capacity, lty = 2, col = "grey")
lines(synth_data$upp ~ synth_data$capacity, lty = 2, col = "grey")
```

Your figure should look like:

```{r Q9.1, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE, fig.height=8, fig.show= ifelse(TRUE, "asis", "hide")}
# Create a fake dataset to feed into our model equation
synth_data <- expand.grid(
  # Set n_staff to median (22 staff)
  n_staff = median(netflix$n_staff), 
  # We have to set policy to either Not implemented or Implemented
  policy = "Implemented",        
  capacity = seq(
    # We create a sequence from the minimum capacity observered
    from = min(netflix$capacity), 
    # To the maximum capacity observed
    to = max(netflix$capacity),   
    # And request 20 values across that range
    length.out = 20))         

# Get both Estimate and Standard Error for each combination of our synthetic data
pred <- predict(mod1, newdata = synth_data, se.fit = TRUE)

# Extract the mean Estimate on response scale
synth_data$pred <- plogis(pred$fit)
# Subtract SE * 1.96 to get 95% and backtransform for lower 95% CI
synth_data$low <- plogis(pred$fit - pred$se.fit * 1.96)
# Add SE * 1.96 to get 95% and backtransform for upper 95% CI
synth_data$upp <- plogis(pred$fit + pred$se.fit * 1.96)

# Plot the raw data
plot(netflix$capacity, netflix$lotr, 
     xlab = "Ward capacity", 
     ylab = "lotr of patients with MRSE")
# And add a line to show our predicted model fit
lines(synth_data$pred ~ synth_data$capacity, lty = 1, col = "black")
lines(synth_data$low ~ synth_data$capacity, lty = 2, col = "grey")
lines(synth_data$upp ~ synth_data$capacity, lty = 2, col = "grey")
```

Getting the `synth_data`, mean estimates and 95% CI is the hard part. Once we have these sorted out, we can create a figure however we like. The example above looks fairly bland and boring, but we can always change this.

Using your code from the above figure showing capacity, adapt it now to visualise the interaction between `policy` and `n_staff` which includes 95% CI. Below I give an example of how I'd visualise the predictions, but you're free to do this however you'd like.

```{r Q9.2, eval=TRUE, echo=SOLUTIONS, results=SOLUTIONS, collapse=TRUE, fig.height=8, fig.show= ifelse(TRUE, "asis", "hide")}
library(ggplot2) # not needed if you already had ggplot loaded
library(scales)  # to show y-axis in figure as percentage (install.packages("scales") if needed)

# Create synthetic data and predictions
synth_data <- expand.grid(n_staff = seq(from = min(netflix$n_staff), to = max(netflix$n_staff), length.out = 20), 
                          policy = c("Implemented", "Not implemented"),
                          capacity = 0.5)      

# Get both Estimate and Standard Error for each combination of our synthetic data
pred <- predict(mod1, newdata = synth_data, se.fit = TRUE)

# Extract the mean Estimate on response scale
synth_data$pred <- plogis(pred$fit)
# Subtract SE * 1.96 to get 95% and backtransform for lower 95% CI
synth_data$low <- plogis(pred$fit - pred$se.fit * 1.96)
# Add SE * 1.96 to get 95% and backtransform for upper 95% CI
synth_data$upp <- plogis(pred$fit + pred$se.fit * 1.96)

# Create the figure
ggplot() +
  geom_point(data = netflix, aes(x = n_staff, y = lotr, colour = policy),
             show.legend = FALSE, size = 0.5) +
  geom_line(data = synth_data, aes(x = n_staff, y = pred, colour = policy),
             show.legend = FALSE) +
  geom_ribbon(data = synth_data, aes(x = n_staff, ymin = low, ymax = upp, fill = policy),
              alpha = 0.3, show.legend = FALSE) +
  facet_wrap(~policy) +
  scale_colour_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = percent, limits = c(0,1)) +
  theme_minimal() +
  labs(x = "Number of netflix staff",
       y = "Predicted lotr\nof MRSE positive patients",
       caption = "Assuming 50% bed occupancy")
```


In my above example figure, notice how predicted relationship for Not Implemented is generally below our data? If you're interesting in why that's the case, try remaking this figure and set capacity (bed occupancy) to 100% to see if that changes anything.

\

### 10. The Monkey's Paw

\

Think back to the very first lecture where we spoke about the four broad assumptions. The first was the assumption of *Validity*. For the data that we've been modelling here, we've constantly spoken about how many patients were MRSE positive. But is that accurate? In truth, we actually don't know how many patients were actually MRSE positive. All we know is how many *tested* positive. In order for a patient to be declared MRSE positive, there are quite a few tests that have to be done, and at minimum number must come back positive before declaring MRSE - meaning lots of tests where we can get false-negatives.

So. What we're actually asking with our models here is not "What lotr of patients are MRSE positive?". Instead, we're asking "What lotr of patients are MRSE positive *and also* tested positive?". This might seem like a trivial difference, but just as in the coffee example in the first lecture, the deterministic process underlying why a sample tests positive or not is more complex than simply the true MRSE state. How reliable are the tests? Was the lab technician tired? Had they just broken up with their partner, and so were very distracted when running the test? Was the result correctly recorded? Plus no shortage of other variables that influenced the probability of whether or not a sample tested positive.

Mathematically, we would say there are two probabilities at play here which combine to give a single probability (and outcome). The probability that a patient is actually MRSE positive is multiplied by the probability MRSE was detected, given they were MRSE positive. With the data we have, it's impossible to disentangle these two probabilities (though there are sampling and corresponding statistical methods to do just that), so we're left explaining the combined probabilities rather than the one we're actually interested in - whether or not a patient is MRSE positive.

This example of violating the assumption of *Validity* is possibly one of the most common ways I see it being broken, and it can be really hard to identify. It can be even harder to convince people they have broken it. Certainly, there are no statistical tests we can perform to test for it, and not much we can do having already collected the data. The only way to catch it, is to 1) know about it, and 2) think *very, very* carefully about what the data actually is. 

This is why the first part of your analysis *must* be sitting back and thinking about your data and research question. Without that step... The Monkey's Paw awaits.

\

### 11. (Optional) Exploring model diagnostics

As in the $Poisson$ exercise, below I include code to simulate a $Binomial$ dataset to allow you to explore the impact of sample size, model misspecification, and effect size, on model diagnostic plots. The *Data Generating Process* (DGP) for the dataset is:

$y_i \sim Binomial(p_i, k_i)$

$logit(p_i) = \beta_0 + \beta_1 \times x_{1,i} + \beta_2 \times x_{2,i} + \beta_3\times x_{3,i} + \beta_4 \times x_{1, i} \times x_{2,i}$

where you are free to decide what the values for the parameters ($\beta_{0,...,4}$) are. Note that given you've now run a model that included an interaction, the DGP also includes an interaction between $x_1$ and $x_2$

Rerun the simulation and analysis varying the sample size (e.g. `N <- 1000`), the effect sizes (e.g. `beta_0 <- 10`) or the formula of the `glm()` (e.g. remove the interaction). Check what effect this has on dispersion and the model diagnostic plots.

```
# Set your seed to have the randomness be cosnsitent each time you run the code
# You can change this, or comment it out, if you want to embrase the randomness
set.seed(1234)

# Set your sample size
N <- 500

# Set the number of trials
k <- 10

# Create three continuous covariates
x1 <- runif(N, 0, 10)
x2 <- runif(N, 0, 10)
x3 <- runif(N, 0, 10)

# Set your parameter values
beta_0 <- 2.5   # This is the intercept (on link scale)
beta_1 <- 0.2   # This is the slope for the x1 variable (on link scale)
beta_2 <- -1.3  # This is the slope for the x2 variable (on link scale)
beta_3 <- 0.4   # This is the slope for the x3 variable (on link scale)
beta_4 <- 0.05  # The combined effect of x1 and x2 (on link scale)

# Generate your linear predictor on the link scale (i.e. log)
# We don't actually need to use log() here (it's already on the link scale)
linear_predictor_link <- beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x1 * x2

# Backtransform your linear predictor to the response scale (i.e. exponentiate)
linear_predictor_response <- plogis(linear_predictor_link)

# Generate your response variable using a Poisson random number generator (rpois)
y <- rbinom(N, size = k, prob = linear_predictor_response)

# Note that the above three lines of code are the equivalent to the equations:
# y ~ Binomial(p, k)
# logit(p) = beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x1 * x2

# Store your data in a dataframe
dat <- data.frame(y, x1, x2, x3)

# Run a Poisson GLM
fit <- glm(y ~ x1 + x2 + x3 + x1 : x2, 
           data = dat, 
           family = poisson)

# See if the model was able to estimate your parameter values 
# and what the dispersion is
summary(fit)

# See how well the model diagnostics perform
par(mfrow = c(2,2))
plot(fit)
```


**End of the Binomial GLM - understanding Multi-drug Resistance *Staphylococcus epidermidis***
